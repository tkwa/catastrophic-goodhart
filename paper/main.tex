%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{mathtools}

% Packages copied from neurips
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % figures
\usepackage{enumitem}
\usepackage{subfiles}       % For long proof of Theorem 5

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Catastrophic Goodhart}
\begin{document}

\twocolumn[
\icmltitle{Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Thomas Kwa}{indp,farlabs}
\icmlauthor{Drake Thomas}{anthropic}
\icmlauthor{Adrià Garriga-Alonso}{farai}

\end{icmlauthorlist}

\icmlaffiliation{indp}{Independent}
\icmlaffiliation{farlabs}{FAR Labs}
\icmlaffiliation{anthropic}{Anthropic}
\icmlaffiliation{farai}{FAR AI}

\icmlcorrespondingauthor{Thomas Kwa}{kwathomas0@gmail.com}
\icmlcorrespondingauthor{Adrià Garriga-Alonso}{adria@far.ai}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
  When applying reinforcement learning from human feedback (RLHF), the reward is learned from data and, therefore, always has some error. It is common to mitigate this by regularizing the policy with KL divergence from a base model, with the hope that balancing reward with regularization will achieve desirable outcomes despite this reward misspecification. We show that when the reward function has light-tailed error, optimal policies under less restrictive KL penalties achieve arbitrarily high utility. However, if error is heavy-tailed, some policies obtain arbitrarily high reward despite achieving no more utility than the base model--a phenomenon we call catastrophic Goodhart. We adapt a discrete optimization method to measure the tails of reward models, finding that they are consistent with light-tailed error. However, the pervasiveness of heavy-tailed distributions in many real-world applications indicates that future sources of RL reward could have heavy-tailed error, increasing the likelihood of reward hacking even with KL regularization.
\end{abstract}

\section{Introduction}

Kullback-Leibler (KL) divergence constraints in reinforcement learning (RL) are employed to stay in regimes where the objective is accurate enough.
Some on-policy \citep{trpo,ppo} and many off-policy \citep{abdolmaleki18mpo,jaques2019way} policy gradient algorithms employ KL constraints or penalties during optimization to prevent the policy from deviating too much from the data collection distribution. This ensures that estimates of each action's advantage are reliable enough to update the policy in a helpful way.

Reinforcement learning from human feedback \citep[RLHF]{rlhf_christiano,ziegler2020finetuning} is a very popular method to induce desirable behavior in language models. RLHF starts with a base pre-trained model, then learns a reward function from human annotator data. Next, it trains an RL policy to maximize this reward, while penalizing high KL divergence from the policy to the base model. RLHF uses an on-policy algorithm and has accurate advantages, but the \emph{reward function} is always somewhat misspecified compared to desired behavior, due to insufficient data, human biases, and other factors.

The main purpose of the KL penalty in RLHF is to limit the consequence of reward modeling errors by keeping the policy within a distribution similar to that on which it was trained. Ideally, in the low-KL regime the reward model's errors are small enough that it provides correct updates to the base model. \Citet{gao2023scaling} empirically supports this view: if the KL divergence in RLHF is allowed to grow too much, with a misspecified reward, the model's performance on the true utility starts to decrease.

We ask: can we obtain good outcomes from misspecified reward in RLHF by controlling the KL divergence? That is, if there is some error between the true reward $V$ and the proxy reward $U$, can the KL help us to still optimize $V$? Using mathematical proof, we answer the question in the negative for heavy-tailed errors: there exist policies which have infinite proxy reward $U$, but whose KL with the base model vanishes (these have undetermined $V$).  We term this phenomenon ``catastrophic Goodhart'', after Goodhart's law.

If the misspecification errors are independent and light-tailed, the KL divergence \emph{does} suffice to guarantee good outcomes. There may also be 
 guarantees under weaker assumptions, but assumptions that intuitively seem sufficient are often not (see Section~\ref{sec:discussion}).

Possibly, other regularization schemes would guarantee good outcomes for heavy-tailed errors, but this is not just a problem of KL. We show that optimizing by conditioning on large reward $U$ has similar outcomes in light- and heavy-tailed regimes.

Empirically, open-source language reward models seem to be light-tailed, which does not imply light-tailed errors but suggests it (Section~\ref{sec:experiments}). However, the errors are likely not independent and, given the prevalence of heavy-tailed distributions in the real world, error in future reward models may also be heavy-tailed. In any case, the present success of RLHF with misspecified rewards cannot be explained solely by the KL regularization in its objective.

%% NEVERMIND, this is kind of obvious as they're imitation-learning methods
% Imitation Learning methods that learn a reward from data then optimize a policy, also attempt to follow a trajectory similar to the expert
% - especially true for learned rewards. E.g. GAIL makes sure the reward is learned where the policy actually goes,  guided-cost learning https://rll.berkeley.edu/gcl/




% - Reward hacking. It's sometimes heavy-tailed. Mention that at least 5 items on Deepmind's list \citep{krakovna2020specification} involve infinite loops or very high scores due to physics glitches and are therefore heavy-tailed.


%% I think we're possibly missing the point, because in practice total reward is going to be limited in magnitude in the KL neighborhood of the original model.
% So the model there is that the magnitude of the reward is A and of the error is B, and then the reward goes up by A in the small KL regime.
% If the error is truly heavy tailed everywhere then yes, the model theoretically can optimize things like this. In practice the errors will be smaller in regions that the pre-trained policy explores

% Another way to think about it: there *exists* a distribution that gets arbitrarily high objective and doesn't get any higher reward. However, in practice when rewards are finite, the optimal policy is well defined and takes actions proportional to prior * exp(reward). so what you get depends on your actual errors, and may be OK (or not).

% I'm still kind of confused about this subject


\section{Background}

\subsection{KL divergence and KL regularization}

Recall that KL divergence between two distributions P and Q is defined as \(
D_{\mathrm{KL}}(P \| Q)=\sum_{x \in \mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right)
\).

If we have two policies $\pi, \pi_0$, we define $D_{KL}(\pi \| \pi_0)$ as the KL divergence between the distributions of actions taken on the states in trajectories reached by $\pi$. That is, if $Tr(\pi)$ is the distribution of trajectories taken by $\pi$, we penalize
\(D_{KL}(\pi \| \pi_0) \triangleq \mathbb E_{s \in T, T\sim Tr(\pi)}[D_{KL}(\pi(s) \| \pi_0(s))]\).

In RLHF, it is common to use the regularization term $\beta D_{KL}\left(\pi \| \pi_0 \right)$ to prevent the learned policy from deviating too much from the base policy, which can prevent unstable behavior or overfitting to the reward model. If our reward model gives reward $U$, then the optimal policy for RLHF with a KL penalty is $
\arg \max_{\pi} \mathbb{E} [U(\pi)]-\beta D_{KL}\left(\pi \| \pi_0 \right).
$

Often the regularization parameter $\beta$ is dynamically adjusted to keep the $D_{KL}$ near some target value \citep{ziegler2020finetuning}.

\subsection{Heavy-tailed distributions}

A distribution $P$ over $\mathbb R$ with cumulative distribution function (CDF) $F_P$ is heavy-tailed if its tail function $\bar F_P(x) \triangleq 1 - F_P(x)$ satisfies \(
\lim _{x \rightarrow \infty} e^{t x} \bar{F}(x)=\infty\) for all $t > 0$. 
Heavy-tailed distributions are well-known in statistics to have a higher probability of producing a single extreme value. For example, if the sum of two independent variables from heavy-tailed distributions is large, it is most likely due to one extreme sample rather than two equally large samples. \citep{wierman2013catastrophes}

\subsection{Reward misspecification and Goodhart's Law\label{sec:goodhart}}

% Some words about reward misspecification

Reward misspecification has caused low-utility outcomes in practice; for example, in \citep{ClarkAmodei2016}, an RL agent trained to play a racing videogame according to a misspecified reward function achieves a high score while failing to complete the course.

\citet{gao2023scaling} introduce the concept of ``overoptimization'': optimizing for a proxy objective decreases performance according to the true objective. This raises the question: in general, when RLHF reward is misspecified, when does the optimal policy produce high utility?

By applying the proxy reward and true reward functions to a distribution over text (generated by an LLM), we get two scalar random variables, which we call $U$ for proxy reward and $V$ for true reward / utility. Then we can define the error in the proxy reward as $X \triangleq U - V$, so that $U = X + V$. Framed this way, optimization for a proxy reward $U$ is a mix of desirable optimization for $V$ and undesirable optimization for $X$. The joint distribution of $V$ and $X$ determines the limiting value of $V$ as we apply more optimization. When we say that reward misspecification can have negative effects, we mean that too much variance in $X$ can "redirect" the optimization pressure from $V$ to $X$, and prevent utility gain from optimization.

Reward misspecification is also studied by \citep{lambert2024alignment}, \citep{laidlaw2024preventing}, and others. Laidlaw et al show that a KL penalty between action distributions can be ineffective, and propose instead regularizing state occupancy measure. Our results show an inherent weakness of KL divergence, including when applied to state occupancy measure.

\section{Theoretical results}

When applying KL regularization, the trained model is regularized towards some base policy $\pi_0$. One would hope that a KL penalty can produce good outcomes even in the case of reward misspecification; that is, if the reward $U$ is the sum of true utility $V$ and an error term $X$, we would hope that optimal policies under a KL penalty achieve high $V$ even if the magnitude of $X$ is large. We show that this is not always the case: Corollary~\ref{cor:heavy-tailed} of Theorems~\ref{thm1}, \ref{thm2}, and~\ref{thm3} establishes that when $X(\pi_0)$ is heavy-tailed, there are arbitrarily well-performing policies $\pi$ with $\mathbb E_{\pi}[V] \approx \mathbb E_{\pi_0}[V]$. However, Theorem~\ref{thm4} shows that when error is light-tailed and independent of $V$, the optimal policy under a KL penalty results in $V > 0$, and $V$ can be made arbitrarily large. Thus, the tails of the error distribution are crucial in determining how much utility will result from optimization towards an imperfect proxy.

Theorems~\ref{thm5} and \ref{thm6} (Section~\ref{sec:theoretical-conditioning} of the appendix) show that the relationship of catastrophic Goodhart to heavy-tailed error is not just a quirk of KL divergence by using a different model of optimization based on conditioning on high reward values. Under this model (and given additional regularity conditions), it is also true that heavy-tailed error results in catastrophic Goodhart, and light-tailed error plus independence results in arbitrarily large utility. All proofs are in the appendix.

\subsection{Heavy-tailed distributions}
\begin{theorem}
\label{thm1}
Given any heavy-tailed reference distribution
\(Q\) over \(\mathbb R\) with mean \(\mu_Q\), and any
\(M, \epsilon > 0\), there is a distribution \(P\) with mean \(\mu_P>M\)
and \(D_{KL}(P \| Q) < \epsilon\).
\end{theorem}

Outline of proof (see appendix for full proof): WLOG take $\mu_Q = 0$. If we set $P_t$ to upweight the probability mass of $Pr_{P_t}(X > t)$ to $c/t$ for some $c, t$, then the mean of $P_t$ will be approximately at least $c$. As $t \to \infty$, the KL divergence $D_{KL}(P_t \| Q)$ will shrink to zero.

\hypertarget{theorem-about-RLHF-with-KL-penalty}{%
\subsection{RLHF with KL penalty under heavy-tailed return distribution}}

We now adapt our result to the case where the policy is a language model and we are training it using RLHF. We are now applying KL divergence over the policies rather than the return distributions. We first formally define the properties of RLHF on language models that cause the result to hold: namely, when when considered as a Markov decision process (MDP), environmental transitions are deterministic and return depends only on the final state reached.

\textbf{\emph{Definition:}} A deterministic-transition MDP with
Markovian returns (DMRMDP) is an MDP \((\mathcal S, \mathcal A, P, R)\)
such that:

\begin{itemize}
\item
  The transition function
  \(P: \mathcal{S} \times \mathcal{A} \to \mathcal{S}\) is
  deterministic, i.e., for each state \(s \in \mathcal{S}\) and action
  \(a \in \mathcal{A}\), there exists a unique state
  \(s' \in \mathcal{S}\) such that \(P(s' | s, a) = 1\). \\ \textbf{In RLHF:} the transition is appending the generated token $a$ to the context $s$.
\item
  There is a set of sink states \(E \subseteq \mathcal S\) that
  terminate a trajectory, which is disjoint from the set of start
  states. \\ \textbf{In RLHF:} The sink states are sequences ending in \texttt{<EOS>} or above a certain length.
\item
  Returns are Markovian; that is, for any two trajectories
  \(\tau=(s_1, a_1, \dots, s_n), \tau'=(s_1', a_1', \dots, s_n'),\) if
  \(s_{n}= s'_{n}\), then \(\tau\) and \(\tau'\) have identical return
  distributions. Equivalently, for the trajectory random variable
  \(T=(S_1, A_1, \dots)\) distributed according to any policy, with
  return \(G\), \(G \bot\!\!\!\!\!\ \bot (S_{<i}, A_{<i}) \ |\ S_i\) for
  any \(i \ge 1\). \\ \textbf{In RLHF:} the return only depends on the full generated string, which is the final state.
\end{itemize}

The language model stochastically outputs the next token $a$ given $s$, and corresponds to the policy.
A DMRMDP is therefore a good model of RLHF.

\begin{theorem}
\label{thm2}
Let \(W = (\mathcal S, \mathcal A, P, R)\) be
a deterministic-transition MDP with Markovian returns. Given \(W\) we
define the function that takes policies to trajectories
\(Tr: (S \to \Delta A) \to \Delta(S \times A)^*\), and the average
return function \(g: (S \times A)^* \to \mathbb R\), which induces a
function \(G: \Delta(S \times A)^* \to \Delta \mathbb R\). Let
\(\pi_0: \mathcal S \to \Delta \mathcal A\) be some base policy. If
\(G \circ Tr(\pi_0)\) is heavy-tailed with finite mean \(\mu_Q\), then
for any \(M, \epsilon > 0\), there is a policy \(\pi\) with mean return
\(\mathbb E[U | U \sim G \circ Tr(\pi)] > M\) and
\(\mathbb E_{s \in T, T\sim Tr(\pi)}[D_{KL}(\pi(s) \| \pi_0(s))] < \epsilon\).
\end{theorem}

\subsection{If V is light-tailed, \texorpdfstring{$\mathbb E_P[V]\! -\! E_Q[V] \to 0$}{} as \texorpdfstring{$D_{KL}\! \to\! 0$}{}}

\begin{theorem}
\label{thm3} If \(V\) is light-tailed and \(d = D_{KL}(P \| Q)\) is bounded, then
\(\mathbb E_P[V]\) is bounded, and \(\mathbb E_P[V] - \mathbb E_Q[V] \to 0\) as
\(d \to 0\).
\end{theorem}

\newtheorem{corr}{Corollary}
\begin{corr}
\label{cor:heavy-tailed}
    Theorems~\ref{thm2} and \ref{thm3} imply that when utility is light-tailed, reward modeling errors make the proxy reward heavy-tailed, and a policy $\pi$ is regularized severely enough to have KL divergence values approaching zero, the reward $\mathbb E[U(\pi)]$ can go to infinity while utility $\mathbb E[V(\pi)]$ approaches a value no higher than the base policy.
\end{corr}

\hypertarget{light-tails-independence-imply-mathbb-ev-to-infty}{%
\subsection{\texorpdfstring{Light-tailed + independence imply
\(\mathbb E[V] \to \infty\)}{Light-tailed + independence imply \textbackslash mathbb EV \textbackslash to \textbackslash infty}}\label{light-tails-independence-imply-mathbb-ev-to-infty}}
 
\begin{theorem}
\label{thm4} If \(U=X+V\) with \(X\) and \(V\) both
light-tailed, and the distribution of U is continuous, and
\(\pi^*(\beta) \triangleq \arg \max_\pi \mathbb E[U(\pi)] - \beta D_{KL}(\pi, \pi_0)\),
then \(\lim_{\beta \to 0^+} \mathbb E[V(\pi^*(\beta))] = \infty\).
\end{theorem}

\section{Experimental Methodology}

Our theoretical results now raise the question of whether the error in reward models is heavy-tailed or light-tailed in practice. \footnote{Note that distributions over a finite set are bounded and cannot be heavy-tailed in a technical sense, and models with a finite context window have a finite input space. We say that a distribution of reward or error is heavy-tailed if it is well-modeled by a heavy-tailed distribution on its support.} If we observe the reward distribution to be light-tailed, this is a strong indication that error is light-tailed. \footnote{It is possible for $U$ to be light-tailed while $X$ and $V$ are both heavy-tailed, but this is unusual and we do not expect it to happen in practice.}

To empirically test whether the reward is heavy-tailed, we consider two lines of evidence: examining the distributions directly through random sampling and temperature-1 sampling, and finding adversarial token sequences that get high rewards. We examine one small and one medium reward model that performed reasonably well on RewardBench \citep{lambert2023rewardbench}. The small model is an OpenAssistant model based on Pythia 1.4B, and the medium model is Starling 7B-alpha \citep{starling2023}\footnote{https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha}.

For random sampling, we sample 30000 length-1024 sequences of uniformly random tokens and observe the distribution of rewards assigned by both Pythia 1.4B and Llama 7B-chat. We also use Llama 7B-chat to generate 16000 length-133 sequences and observe the distribution of rewards assigned by Starling 7B-alpha.

Because sampling is inefficient at probing the extreme tail, we also find token sequences that optimize Starling 7B-alpha for reward. We considered Greedy Coordinate Gradient (GCG) from \citep{zou2023universal}, a method used to find adversarial suffixes that circumvent jailbreaking, but decided on a faster version of GCG called Accelerated Coordinate Gradient (ACG) from \citep{haizelabs2024acg}. See Table \ref{table1} for ACG hyperparameters.

Generating plots took about 5 GPU-hours on 1x Nvidia H100, and running ACG took a further 8 hours.

\section{Results\label{sec:experiments}}

When sampling token sequences, both the Pythia model on random inputs (Figure \ref{fig:pythia-random}) and Starling 7B-alpha on Llama-generated inputs (Figure \ref{fig:starling-llama}) appear approximately normal and, therefore, light-tailed. Starling on random inputs (Figure \ref{fig:starling-random} is ambiguous, with the exponential Q-Q plot having an outlier that could indicate a heavy-tailed distribution, but the Hill estimator is consistent with a light-tailed distribution. Because Llama-7B-chat is a more reasonable base model than a completely random policy, we believe that Starling 7B-alpha is more likely to be light-tailed for the purposes of our theoretical results.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/reward_plots_random_starling_30k_se.png}
    \caption{Plots of the distribution of reward from 30000 random length-1024 token sequences to Starling 7B-alpha. Clockwise from top left: The histogram shows a unimodal distribution with a slight right skew. The normal probability plot indicates the data are heavier-tailed than normal. The Hill estimator (error bars are standard error) appears to be 0.20 for higher values but fluctuates for lower values. The exponential probability plot of the right half of the distribution is consistent with either light or heavy tails (under heavy tails, the slope would go to infinity).}
    \label{fig:starling-random}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/reward_plots_llama_starling_16k.png}
    \caption{Plots of the reward distribution from 16000 token sequences generated by Llama 7B-chat of length $\le 133$, starting with five random tokens. Clockwise from top left: A histogram shows the reward distribution has a left skew. The normal probability plot suggests reward is approximately normal and thus light-tailed. The Hill estimator plot should stabilize if the distribution is heavy-tailed, but it does not; thus, there is no evidence the distribution is heavy-tailed. The exponential probability plot also indicates light tails, because the curve is bending downwards. }
    \label{fig:starling-llama}
\end{figure}

The ACG results need some interpretation. The KL divergence between two distributions $P$ and $Q$ if $P$ is the same as $Q$ a fraction $1-\alpha$ of the time, but is some value $x$ a fraction $\alpha$ of the time is given by \(D_{KL}(P \| Q) = \left[(1-\alpha)q(x) + \alpha\right] \log \left(\frac{(1-\alpha)q(x) + \alpha}{q(x)}\right) + 
(1-\alpha) \log (1-\alpha) (1 - q(x))\).


When $\alpha$ is small but much larger than $q(x)$, we approximate this to first order as $D_{KL}(P \| Q) \approx \alpha \log \left(\frac{\alpha}{q(x)}\right)$. In Theorems 1 and 2, we prove that when the error is sufficiently heavy-tailed, a policy that gets extremely large reward a small fraction of the time will achieve high expected reward with low KL divergence. This is not the case here because the rewards achieved through ACG were small and the log-probabilities extremely negative. For example, a policy that matches Llama 2-chat's base reward 99\% of the time and uses the highest-reward input generated by ACG $\alpha=$1\% of the time will have KL divergence from Llama 2-chat of $\alpha(\log(\alpha) - 1339.70) = 13.35$ nats, but reward only about $\alpha*(2.2377-0.3329) = 0.02571$ greater than the base model, far less than can be obtained with the same KL divergence by conditioning.

\section{Discussion and Limitations\label{sec:discussion}}

\subsection{How likely is catastrophic Goodhart?}

The low-KL policies that result in catastrophic Goodhart are not a unique optimal policy, just one family of high-performing policies. When optimizing $\mathbb{E} [U(\pi)]-\beta D_{K L}\left(\pi, \pi_0 \right)$, the outcome depends on RL training dynamics; it could be that $D_{KL} \to 0$ causing catastrophic Goodhart, but more likely both terms will go to infinity, potentially allowing $V \to \infty$. Catastrophic Goodhart can be prevented by using a light-tailed or bounded reward function.

Even so, catastrophic Goodhart is likely to occur in many scenarios where KL regularization is naively employed in an attempt to avoid Goodhart’s Law:

\begin{itemize}
    \item If we maximize $\sigma(\mathbb E[U]) + D_{KL}(Tr(\pi) \| Tr(\pi_0))$, where $\sigma$ is a bounded function (e.g. sigmoid), all near-optimal policies will have $V \approx 0$. Since we can only obtain so much reward from $\sigma(\mathbb{E}[U])$, it pays to make the KL (and thus V) go to zero.
    \item If we cap KL to a finite value (or dynamically adjust the KL penalty to target a finite KL, as done in \citet{ziegler2020finetuning}, then $\mathbb E[V]$ is also upper bounded by a finite value (see Theorem 3), and we think it is likely that $\mathbb E[V] \approx 0$. Consider a toy model where an AI can adjust three parameters: true quality $V$ of responses, frequency of reward hacking (producing actions with extremely high X), and severity of hacking (value of X on those actions). Adjusting the policy to increase $\mathbb E[U]$ without increasing KL increase the severity of hacking while decreasing either frequency of hacking or quality of responses. When $E[U]$ is already large, decreasing quality has much better returns than decreasing frequency. This is similar to Theorems \ref{thm5}, \ref{thm6} about hard-threshold optimization.
    \item Any way we maximize $\mathbb{E} [U(\pi)]-\beta D_{K L}\left(\pi, \pi_0 \right)$ results in very large values of $\mathbb E[U(\pi)]$, and there are a number of arguments that extreme optimization for an imperfect proxy can result in decreased utility due to tradeoffs between $X$ and $V$; e.g., the constrained resource scenario in \citep{zhuang2021consequences}.
\end{itemize}

\subsection{Independence assumptions}

Theorems 2 and 3 do not require any independence assumption, but Theorems 4, 5, and 6 require that error $X$ and utility $V$ are independent, which seems to be violated in practice. Future work could weaken this assumption, although intuitively obvious ways to weaken it result in the statement being false. \footnote{Suppose that error $X$ is light-tailed conditional on any value of $V$, but our proxy is merely unbiased ($\mathbb E[X|V=v]=0$ for all $v$). Then the limit of $V$ under optimization for $X+V$ still depends on the relationship between $X$ and $V$. If they are independent, Theorem 6 says that \(\lim_{t \to\infty} \mathbb E[V | X + V \ge t] = \infty\). But if \(V \sim N(0, 1)\), and \(X | V \sim N(0, 4)\) when \(V \in [-1, 1]\), otherwise \(X=0\), then \(\lim_{t \to\infty} \mathbb E[V | X + V \ge t] = 0\).} 

\subsection{Stronger optimization methods}

We did not search the entire space of token sequences, so we cannot rule out that the reward is heavy-tailed enough to cause catastrophic Goodhart in some situations. While it is intractable to search the more than $10^{2000}$ possible token sequences, future work could get more evidence through more powerful optimization methods.

\subsection{Relation to previous overoptimization work}

\citet{gao2023scaling} found that optimizing the reward of small reward models causes overoptimization: a decrease in utility with increasing optimization. However, we observed that reward models are light-tailed, and (Theorem \ref{thm4}) that independence combined with light-tailed error prevents overoptimization. We think this discrepancy is explained by dependence between error and utility. Policies optimized for high error may activate features in the proxy reward models that are undesirable according to the true utility function.\footnote{There are other explanations possible. Perhaps better optimization methods would find heavy-tailed reward in open reward models; or OpenAI's reward models have heavy-tailed error (and their results are straightforwardly explained by our Theorem~\ref{thm1}), while open reward models have light-tailed error.}
More research is needed to understand why high-error completions have low utility and to design reward models that do not suffer from this problem; Perhaps it is possible to construct reward models whose errors are in directions orthogonal to human preferences, so that the large-reward completions do not have lower utility.

\section{Conclusion}

We have argued that the purpose of the KL divergence regularization in RLHF is to mitigate reward misspecification. However, we have also proven that when errors in the reward function are heavy-tailed, it cannot serve this purpose: even with zero KL divergence, there are policies that achieve very high misspecified reward and no actual reward.

When errors are light-tailed and independent, the KL divergence can mitigate misspecification, but when they are dependent, this may not be possible. Thus, we must look to places other than the KL objective to explain the current success of RLHF and ensure its continued success in the future.

% Acknowledgements are not in submission due to anonymity
\section*{Impact Statement}

As this work aims to improve the safety of future ML systems by characterizing a possible failure mode of reward misspecification in RLHF, we hope the social impact is positive. We see no particular ethical issues to discuss.

\small{
\bibliography{refs.bib}
\bibliographystyle{icml2024}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\onecolumn

\hypertarget{appendix}{%
\section{Proofs}\label{appendix}}

\subsection{Theorem 1}
\newtheorem*{theorem1}{Restatement of Theorem \ref{thm1}}
\begin{theorem1} Given any heavy-tailed reference distribution
\(Q\) over \(\mathbb R\) with mean \(\mu_Q\), and any
\(M, \epsilon > 0\), there is a distribution \(P\) with mean \(\mu_P>M\)
and \(D_{KL}(P \| Q) < \epsilon\).
\end{theorem1}

Intuitively, in a heavy-tailed distribution, events with extremely high $x$ are not very rare, so you don’t pay much of a KL penalty to upweight them so they happen about $1/x$ of the time. This is visually illustrated in Figure \ref{fig1}.

\begin{figure}
    \label{fig1}
    \centering
    \includegraphics[width=0.3\linewidth]{goodhart_kl_001.png}
    \includegraphics[width=0.3\linewidth]{goodhart_kl_024.png}
    \includegraphics[width=0.3\linewidth]{goodhart_kl_050.png}
    \caption{As $t \to \infty$, the mean of $X$ (blue bar) grows without bound while KL divergence $D_{KL}(P_t \,\|\, Q)$ (orange bar) goes to 0. The base distribution Q is a Student t-distribution with $df=3$. In this case, high values of X are upweighted to $1/t^{0.8}$; upweighting them to $1/t$ would cause $\mathbb E[X]$ to converge to ~$1$ while KL divergence goes to zero faster.}
\end{figure}

\emph{Proof.} WLOG let \(\mu_Q = 0\). We construct a sequence of
distributions \(\{P_t\}\) such that
\(\lim_{t \to \infty} \mathbb E_{P_t}[X] \ge c\) for any constant \(c\),
and \(\lim_{t \to \infty} D_{KL}(P_t \| Q) = 0\). We define \(P_t\) for
any \(t > c\) thusly. Writing \(F_{P_t}(x)\) for the CDF
\(Pr_{X \sim P_t}(X \le x)\) and \(\bar F_{P_t}(x)\) for
\(1 - F_{P_t}(x)\), we let

$$ \bar F\_\{P\_t\}(x) =
\begin{cases} 1 - \frac{1 - c/t}{F_Q(t)}F_Q(x) & x \le t
\\ \frac{c/t}{\bar F_Q(t)}\bar F_Q(x) & x > t
\end{cases}
$$

Intuitively, we rescale the part of the distribution to the right of
\(t\) evenly to have total probability \(c/t\), which is less than 1
because \(t > c\).

We must check that \(\lim_{t \to \infty} \mathbb E_{P_t}[X] = c\). We
can write

\begin{align*} \mathbb E_{P_t}[X] = F_{P_t}(t) \mathbb E_{P_t}[X | X \le t] &+ \bar F_{P_t}(t) \mathbb E_{P_t}[X | X > t]    \\ = F_{P_t}(t) \mathbb E_Q[X | X \le t] &+ \bar F_{P_t}(t) \mathbb E_Q[X | X > t]    \\ = F_Q(t) \mathbb E_Q[X | X \le t] &+ \bar F_Q(t) \mathbb E_Q[X | X > t] + \\ (F_{P_t}(t) - F_Q(t))&E_Q[X | X \le t] + (\bar F_{P_t}(t) - \bar F_Q(t)) E_Q[X | X > t]     \\ = \mathbb E_Q[X] + (\bar F_{P_t}(t) - &\bar F_Q(t))(E_Q[X | X > t] - E_Q[X | X \le t])
\end{align*}

We know that \(\mathbb E_Q[X | X > t] > t\) because it is an integral of
values strictly greater than t. Because \(\mathbb E_Q[X] = 0\) is a
weighted average of \(\mathbb E_Q[X|X>t]\) and \(E_Q[X | X \le t]\), and
\(\mathbb E_Q[X|X>t] > 0\), we know \(E_Q[X | X \le t] < 0\). So
\(E_Q[X | X > t] - E_Q[X | X \le t] > t.\) We also know that for
sufficiently large \(t\), \((F_{P_t}(t) - F_Q(t)) > 0\). Intuitively,
starting from \(Q\), which has mean 0, \(P_t\) moves a probability mass
approaching \(\frac c t\) from mean \textless0 to mean \textgreater t.

Now we can say

\[
\lim_{t \to \infty} \mathbb E_{P_t}[X] > \lim_{t \to \infty}\left[ \mathbb E_Q[X] + (\bar F_{P_t}(t) - \bar F_Q(t))(t-0) \right]
\\ = \lim_{t \to \infty}\left( \frac c t - \bar F_Q(t) \right) t = \lim_{t \to \infty}c - t \bar F_Q(t)
\]

Because \(Q\) has a finite mean,
\(\lim_{t \to \infty} t \bar F_Q(t) = 0\), and so
\(\mathbb \lim_{t \to \infty} \mathbb E_{P_t}[X] \ge c\).

Now we check that \(\lim_{t \to \infty} D_{KL}(P_t \| Q) = 0\):

\begin{align*}D_{KL}({P_t} \| Q) &= \int_{\mathbb R} \log \frac{{P_t}(dx)}{Q(dx)} \,{P_t}(dx)\\&= \int_{x \le t} \log \frac{{P_t}(dx)}{Q(dx)} \,{P_t}(dx) + \int_{x > t} \log \frac{{P_t}(dx)}{Q(dx)} \,{P_t}(dx)\\ &= F_{P_t}(t) \log \frac{F_{P_t}(t)}{F_Q(t)} + \bar F_{P_t}(t) \log \frac{\bar F_{P_t}(t)}{\bar F_Q(t)} \text{\quad since both ratios are constant}\\ &= F_{P_t}(t) \log \frac{1 - c/t}{F_Q(t)} + \bar F_{P_t}(t) \log \frac{\bar F_{P_t}(t)}{\bar F_Q(t)}\end{align*}

Since both \(1-c/t\) and \(F_Q(t)\) go to \(1\) as \(t \to \infty\), the
left term goes to \(0\), and so

\begin{align*}\lim_{t \to \infty} D_{KL}(P_t \| Q)&\le 0 + \lim_{t \to \infty}\bar F_{P_t}(t) \log \frac{\bar F_{P_t}(t)
}{\bar F_Q(t)}
\\ & = \lim_{t \to \infty}\frac c t \log \frac c {t \bar F_Q(t)} \le \lim_{t \to \infty} \frac c t \log \frac 1 {\bar F_Q(t)} 
\\&= \lim_{t \to \infty}-\frac c t \log \bar F_Q(t) \text{\quad since t>c}
\end{align*}

\(Q\) is heavy-tailed, so by definition
\(\lim _{t \rightarrow \infty} e^{a t} \bar{F}_Q(t)=\infty \quad \text { for all } a>0\).
This implies that for every \(a > 0\) there is a sufficiently large
\(t_{c}\) so that for all \(t > t_c\), \(\bar F_Q(x) > e^{-at}\), which
means that \(\log \bar F_Q(t) > -a t\).

Therefore for every \(a > 0\),
\(\lim_{t \to \infty} D_{KL}(P_t \| Q) \le \lim_{t \to \infty} -c/t \log \bar F_Q(t) < \lim_{t \to \infty} -\frac {-act} t = ac\),
which since KL divergence is nonnegative means
that\(\lim_{t \to \infty} D_{KL}(P_t \| Q) = 0\) as desired.
\(\blacksquare\)

\subsection{Theorem 2}
\newtheorem*{theorem2}{Restatement of Theorem \ref{thm2}}
\begin{theorem2}
Let \(W = (\mathcal S, \mathcal A, P, R)\) be
a deterministic-transition MDP with Markovian returns. Given \(W\), we
define the function that takes policies to trajectories
\(Tr: (S \to \Delta A) \to \Delta(S \times A)^*\), and the average
return function \(g: (S \times A)^* \to \mathbb R\) which induces a
function \(G: \Delta(S \times A)^* \to \Delta \mathbb R\). Let
\(\pi_0: \mathcal S \to \Delta \mathcal A\) be some base policy. If
\(G \circ Tr(\pi_0)\) is heavy-tailed with finite mean \(\mu_Q\), then
for any \(M, \epsilon > 0\), there is a policy \(\pi\) with mean return
\(\mathbb E[U | U \sim G \circ Tr(\pi)] > M\) and
\(\mathbb E_{s \in T, T\sim Tr(\pi)}[D_{KL}(\pi(s) \| \pi_0(s))] < \epsilon\).
\end{theorem2}

\emph{Proof:} We will exhibit a distribution of trajectories \(\rho\)
such that \(D_{KL}(\rho \| Tr(\pi_0)) < \epsilon\) and
\(\mathbb E[G(\rho)] > M\), and then construct a policy \(\pi\) with
\(Tr(\pi) = \rho\). Note that this proof applies for continuous action
spaces if trajectories are replaced with measurable sets, but this would
make it harder to read.

Let \(\rho_{\pi_0} = Tr(\pi_0)\). We have a heavy-tailed distribution of
return \(Q \triangleq G(\rho_{\pi_0})\) over \(\mathbb R\), so we can
apply Theorem~\ref{thm1}. But to define \(\rho\), we can construct \(P_t\) in the
proof of Theorem~\ref{thm1} in a particular way. For any \(t>c\), we need a
\(P_t\) that uniformly upweights values of mean return such that
\(\bar F_{P_t}(t) = c/t\). We can define \(\rho_t\) such that any
trajectory \(\tau\) is upweighted by a factor depending only on its mean
return:

\[
\rho_t(\tau) = \begin{cases} \frac{1 - c/t}{F_Q(t)} \rho_{\pi_0}(\tau) & g(\tau) \le t
\\ \frac{c/t}{\bar F_Q(t)}\rho_{\pi_0}(\tau) & g(\tau) > t
\end{cases}
\]

Then we can let \(P_t \triangleq G \circ \rho_t\) and the rest of the
proof of Theorem~\ref{thm1} applies. Therefore, applying the theorem, we can let
\(\rho = \rho_t\) for sufficiently large \(t\), and then
\(\mu_{G \circ \rho} > M\) and
\(D_{KL}(G \circ \rho, G \circ \rho_{\pi_0}) < \epsilon\). By the
chain rule for KL divergence,
\(D_{KL}(\rho, \rho_{\pi_0}) = D_{KL}(G\circ \rho, G \circ \rho_{\pi_0}) + \mathbb E_{\gamma \sim G\circ\rho}[D_{KL}(\rho(T) | G(T)=\gamma \ \|\ \rho_{\pi_0}(T) | G(T)=\gamma)]\).
Since we constructed \(\rho\) so that the probabilities of each \(\tau\)
conditional on its return being \(\gamma\) are equal, the second term is
zero, and we also have \(D_{KL}(\rho, \rho_{\pi_0}) < \epsilon\).

Finally, since the KL divergence between trajectory distributions is the
sum of KL divergence between policies at each action in the trajectory,
and each trajectory has at least one action,
\(\mathbb E_{s \in T, T\sim Tr(\pi)}[D_{KL}(\pi(s) \| \pi_0(s))] \le \mathbb E_{T\sim Tr(\pi)} \sum_{s \in T}[D_{KL}(\pi(s) \| \pi_0(s))] = D_{KL}(\rho \| \rho_{\pi_0}) < \epsilon\)
as desired.

To define \(\pi\) such that \(Tr(\pi) = \rho\), we let
\(\pi(s, a) = Pr(a_i = a | \tau = (..., s, a_i, ...) \sim \rho)\).

Then, the probability that any trajectory
\(\tau = (s_1, a_1, \dots, a_n)\) is sampled is:

\begin{align}
Tr(\pi)(\tau) &= \prod_{i=1}^n \pi(s_i, a_i) 
\\&= \prod_{i=1}^n Pr(a_i=a_i' | \tau' = (..., s, a_i', ...) \sim \rho)
\\&= \prod_{i=1}^n Pr(a_i = a_i' | \tau' = (s_1', a_1', ..., s, a_i', ...) \sim \rho, s_{<i} = s'_{<i}, a_{<i} = a'_{<i})
\\&= \rho(\tau)
\end{align}

In (2), returns are Markovian, so all trajectory prefixes ending in
state \(s\) have the same distribution of returns under any policy. In
the construction of \(\rho\), all trajectories with the same mean return
have equal measure. Therefore, conditioning on earlier states and
actions of \(\tau\) does not change the measure, so we can write (3). So
\(Tr(\pi)=\rho\) as desired. \(\blacksquare\)

\subsection{Theorem 3}
\newtheorem*{theorem3}{Restatement of Theorem \ref{thm3}}
\begin{theorem3} If \(V\) is light-tailed, \(\mathbb E_Q[V]\)
is zero, and \(d = D_{KL}(P \| Q)\) is bounded, then
\(\mathbb E_P[V]\) is bounded, and \(\mathbb E_P[V] \to 0\) as
\(d \to 0\).
\end{theorem3}

\emph{Proof.} Using Lagrange multipliers, we find that when KL divergence is
minimized, we have
\(P(V) [\lambda_1 \log \frac{P(V)}{Q(V)} + \lambda_2 - X] = 0\) for some
constants \(\lambda_1, \lambda_2\), so

\begin{align}\log \frac{P(V)}{Q(V)} = \frac{V - \lambda_2}{\lambda_1}
\\ P(V) = Q(V)\exp\left(\frac{V - \lambda_2}{\lambda_1}\right) = Q(V)
\\ e^{V/\lambda\_1} e^{-\lambda\_2/\lambda\_1} = C Q(V)
e^{V/\lambda\_1}
\end{align}

That is, the new PDF is an
\href{https://en.wikipedia.org/wiki/Exponential_tilting}{exponential
tilting} of the old PDF. Now, what is \(\mathbb E_P[V]\)? It's just
\(\int_{-\infty}^{\infty} C V e^{V/\lambda_1} Q(X) \,dV\). If the
distribution of V is heavy-tailed distribution, this is \(\infty\); if
it is light-tailed, this is some finite value.

When \(d = 0\), \(P\) and \(Q\) are identical, and \(\mathbb E[V] = 0\).
So by a continuity argument, \(\mathbb E_P[V] \to 0\) as \(d \to 0\).
\(\blacksquare\)

\subsection{Theorem 4}

\newtheorem*{theorem4}{Restatement of Theorem \ref{thm4}}
\begin{theorem4}
If \(U=X+V\) with \(X\) and \(V\) both
light-tailed, and the distribution of U is continuous, and
\(\pi^*(\beta) \triangleq \arg \max_\pi \mathbb E[U(\pi)] - \beta D_{KL}(\pi, \pi_0)\),
then \(\lim_{\beta \to 0^+} \mathbb E[V(\pi^*(\beta))] = \infty\).
\end{theorem4}

\emph{Proof.} Fix some \(\beta\). Using Lagrange multipliers, we find
that for any event \(S\),
\(\Pr_\pi(S) = \Pr_{\pi_0}(S) e^{\lambda U(S)}\). Let \(c(\beta)\) be
the median value of \(U\) under the policy \(\pi^*(\beta)\); that is,
\(Pr(U > c(\beta) | U \sim G \circ Tr(\pi^*(\beta))) = \frac 1 2.\) This
exists because \(U\) has a continuous distribution. Then:

\begin{align*}E[V | \pi] &= \frac 1 2 E[V | \pi, U < c] + \frac 1 2 E[V | \pi, U \ge c]
\\ &\ge \frac 1 2 E[V | \pi, U < c] + \frac 1 2 E[V | \pi]
\\ \lim_{\beta \to 0^+} E[V | \pi] &\ge \lim_{\beta \to 0^+} \frac 1 2 E[V | \pi, U < c] + \lim_{\beta \to 0^+} \frac 1 2 E[V | \pi]
\end{align*}

The left term is \(c\), while the right term is \(\infty\), so the
overall limit is \(\infty\).

\section{Conditioning as alternate model of optimization\label{sec:theoretical-conditioning}}

Although we think a KL divergence penalty or cap is the most realistic setting for RLHF, it is not the only model of optimization where heavy-tailedness of the error determines whether catastrophic Goodhart occurs. Consider another model of optimization where $U = X+V$ as before, but we simply condition on $U$ being higher than some threshold $t$.\footnote{This could model a satisficing agent that takes random acceptable actions.} In this case, we are interested in the quantity $\lim_{t \to \infty} \mathbb E[V | X + V \ge t]$. If we slightly strengthen the heavy-tailedness and light-tailedness assumptions, heavy-tailed error results in catastrophic Goodhart, while light-tailed error results in arbitrarily high expected utility.

\subsection{Conditioning with heavy-tailed error produces catastrophic Goodhart}

\begin{theorem}
    \label{thm5} Let $X$ and $V$ be two independent random variables with CDFs $F_X$ and $F_V$ and tail functions $\bar F_V \triangleq 1 - F_V$, $\bar F_X \triangleq 1 - F_X$ such that
    \begin{itemize}
        \item $V$ has a finite mean.
        \item $X$ is subexponential; that is, $\lim_{x\to\infty}\frac{\text{Pr}(X_1+X_2>x)}{\text{Pr}(X>x)} = 2$ if $X_1, X_2$ are two independent samples from $X$. This is a slightly stronger property than being heavy-tailed.
        \item The tail of $V$ is sufficiently lighter than the tail of $X$ such that \(\lim_{t\to\infty}\frac{t^p\bar F_V(t)}{\bar F_X(t)} = 0\) for some \(p > 1\).
    \end{itemize}
    Then $\lim_{t \to \infty} \mathbb E[V | X + V \ge t] =\mathbb{E}[V]$; that is, catastrophic Goodhart occurs in the limit of optimization for $U=X+V$.
\end{theorem}

The proof requires expressing the conditional expectation in question as $\frac{\int_{-\infty}^\infty vf_V(v)\text{Pr}(X>t-v)} {\int_{-\infty}^\infty f_V(v)\text{Pr}(X>t-v)}$, then partitioning the interval $(-\infty, \infty)$ into four regions and bounding the integrand in the numerator above by a different quantity in each region. 

\subfile{supplementary/theorem5}

\subsection{Conditioning with light-tailed error produces arbitrarily high utility}
\begin{theorem}
    \label{thm6}
    Let $X, V$ be independent random variables such that $\lim_{t\to\infty}\frac{\bar{F}_X(t+1)}{\bar{F}_X(t)}=0$. (This implies that X has tails that are dominated by $e^{-cx}$ for any c, though it's a slightly stronger claim because it requires that X not have large jumps in the decay of its tails.)
    Then for any V with a finite mean which has no upper bound, $\lim_{t\to\infty}\mathbb{E}[V|X+V > t] = \infty$. 
\end{theorem}

Theorem \ref{thm6} generalizes a consequence of the "Regressional Goodhart Identity" in \citep{gao2023scaling}.

\begin{proof}
Let \(\text{Pr}(V>c+1)=p>0\), which exists by our assumption that \(V\) is unbounded.

Let \(\mathbb E[V|V<c] = q\). (If this is undefined because the conditional has probability \(0\), we'll have the desired result anyway since then \(V\) would always be at least \(c\).) 

Observe that for all \(t\), \(\mathbb E[V|V<c, X+V>t] \ge q\) (assuming it is defined), because we're conditioning \((V|V<c)\) on an event which is more likely for larger \(v\) (since \(X\) and \(V\) are independent). 

First, let's see that \(\lim_{t\to\infty}\frac{P(V<c|X+V\ge t)}{P(V>c+1|X+V\ge t)}=0\). This ratio of probabilities is equal to

\(\frac{\int_{-\infty}^c f_V(v)\bar F_X(t-v)}{\int_{c+1}^\infty f_V(v)\bar F_X(t-v)} \le \frac{\int_{-\infty}^c f_V(v)\bar F_X(t-c)}{\int_{c+1}^\infty f_V(v)\bar F_X(t-c-1)} = \frac{\bar F_X(t-c)}{\bar F_X(t-c-1)}\cdot \frac{\int_{-\infty}^c f_V(v)}{\int_{c+1}^\infty f_V(v)}\)

\(=\frac{\bar F_X(t-c)}{\bar F_X(t-c-1)}\cdot \frac{\text{Pr}(V<c)}{\text{Pr}(V>c+1)}\le \frac{\bar F_X(t-c)}{\bar F_X(t-c-1)}\cdot \frac1p\)

which, by our assumption that \(\lim_{t\to\infty}\frac{\bar{F}_X(t+1)}{\bar{F}_X(t)}=0\), will get arbitrarily small as \(t\) increases for any positive \(p\).

Now, consider \(\mathbb E[V|X+V\ge t]\). We can break this up as the sum across outcomes \(Z\) of \(\mathbb E[V|Z,X+V\ge t]\cdot \text{Pr}(Z | X+V\ge t)\) for the three disjoint outcomes \(V<c\), \(c\le V\le c+1\), and \(V>c+1\). Note that we can lower bound these expectations by \(q, c, c+1\) respectively. But then once \(t\) is large enough that \(\frac{\text{Pr}(V<c|X+V\ge t)}{\text{Pr}(V>c+1|X+V\ge t)}<\frac1{c-q}\),  this weighted sum of conditional expectations will add to more than \(c\).
\end{proof}

\section{Additional figures}

See figures \ref{fig:pythia-random}, \ref{fig:acg-results}.

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{reward_hist_random_pythia.png}
    \includegraphics[width=0.4\linewidth]{reward_qq_random_pythia.png}
    \caption{Histogram and normal probability plot of reward assigned by Pythia RM to random length-1024 token sequences. The Q-Q plot suggests the distribution is approximately normal, which is much lighter-tailed than exponential.}
    \label{fig:pythia-random}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/acg_results_40.png}
    \caption{Reward and log-probability for ACG-optimized inputs to Starling 7B-alpha.}
    \label{fig:acg-results}
\end{figure}

\section{Hyperparameters for ACG}

See table \ref{tableh}.

\begin{table}
    \centering
    \caption{Hyperparameters for ACG}
    \label{tableh}
    \begin{tabular}{|c|c|}
        \hline
        Parameter & Value \\
        \hline
        Context length & 133 \\
        \hline
        Iterations & 1000 \\
        \hline
        Candidates per seq. position (k) & 3 \\
        \hline
        Annealing starting value & 9 \\
        \hline
        Annealing ending value & 2 \\
        \hline
    \end{tabular}
\end{table}

\section{Assets}

We use three models for our experiments: Starling 7B-alpha, Llama 2 7B-chat, and Pythia-1.4B. Starling was developed by Berkeley, and Pythia by EleutherAI. Starling and Pythia models are licensed under Apache-2.0.\footnote{https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha} \footnote{https://huggingface.co/EleutherAI/pythia-1.4b} Llama 2 models were developed by Meta and licensed under a license published by Meta.\footnote{https://ai.meta.com/llama/license/} 

% Acknowledgements should only appear in the accepted version.


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
