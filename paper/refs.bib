@article{amodei2016concrete,
  author        = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man\'{e}, Dan},
  archiveprefix = {arXiv},
  eprint        = {1606.06565},
  title         = {Concrete problems in {AI} safety},
  year          = {2016},
}

@misc{ClarkAmodei2016,
  author = {Jack Clark and Dario Amodei},
  title = {Faulty Reward Functions in the Wild},
  year = {2016},
  url = {https://openai.com/index/faulty-reward-functions/},
  organization = {OpenAI},
  note = {Accessed: 2024-07-07},
  month = dec,
  day = {21}
}


@book{foss2013introduction,
  title         = {An Introduction to Heavy-Tailed and Subexponential Distributions},
  author        = {Foss, Sergey and Korshunov, Dmitry and Zachary, Stan},
  year          = {2013},
  publisher     = {Springer},
  edition       = {2},
  doi           = {10.1007/978-1-4614-7101-1},
  url           = {https://link.springer.com/book/10.1007/978-1-4614-7101-1},
}
@inproceedings{gao2023scaling,
  title         = {Scaling Laws for Reward Model Overoptimization},
  author        = {Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  pages         = {10835--10866},
  year          = {2023},
  editor        = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume        = {202},
  series        = {Proceedings of Machine Learning Research},
  month         = {23--29 Jul},
  publisher     = {PMLR},
  pdf           = {https://proceedings.mlr.press/v202/gao23h/gao23h.pdf},
  url           = {https://proceedings.mlr.press/v202/gao23h.html},
  abstract      = {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ``gold-standard'' reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.},
}
Here is the BibTeX entry for the blog post:
@misc{haizelabs2024acg,
  author        = {{Haize Labs}},
  title         = {Making a SOTA Adversarial Attack on LLMs 38x Faster},
  howpublished  = {\url{https://blog.haizelabs.com/posts/acg/}},
  year          = {2024},
  month         = mar,
  note          = {Accessed: 2024-05-22},
}
@misc{krakovna2020specification,
  title         = {Specification gaming: the flip side of AI ingenuity},
  author        = {Krakovna, Victoria and Uesato, Jonathan and Mikulik, Vladimir and Rahtz, Matthew and Everitt, Tom and Kumar, Ramana and Kenton, Zac and Leike, Jan and Legg, Shane},
  year          = {2020},
  month         = apr,
  day           = {21},
  howpublished  = {\url{https://deepmindsafetyresearch.medium.com/specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4}},
  note          = {DeepMind Safety Research, Medium},
}

@article{laidlaw2024preventing,
  title = {Preventing Reward Hacking with Occupancy Measure Regularization},
  author = {Cassidy Laidlaw and Shivam Singhal and Anca Dragan},
  year = {2024},
  journal = {arXiv},
  eprint = {2403.03185},
  primaryClass = {cs.AI},
  url = {https://www.arxiv.org/abs/2403.03185},
  note = {Accessed: 2024-07-07}
}

@misc{lambert2023rewardbench,
  title         = {RewardBench: Evaluating Reward Models for Language Modeling},
  author        = {Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
  year          = {2023},
  eprint        = {2403.13787},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  note          = {40 pages, 19 figures, 12 tables},
  url           = {https://arxiv.org/abs/2403.13787},
  doi           = {10.48550/arXiv.2403.13787},
}

@misc{lambert2024alignment,
    author = {Nathan Lambert and Roberto Calandra},
    title = {The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback},
    year = {2024},
    eprint = {2311.00168v2},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    institution = {Allen Institute for AI, Berkeley, CA, USA and TU Dresden, Dresden, Germany},
    email = {nathanl@allenai.org, roberto.calandra@tu-dresden.de},
    month = {feb}
}

@article{zhuang2021consequences,
  title         = {Consequences of Misaligned {AI}},
  author        = {Simon Zhuang and Dylan Hadfield-Menell},
  year          = {2021},
  journal       = {Advances in Neural Information Processing Systems},
  volume        = {33},
  pages         = {15762--15773},
  publisher     = {Curran Associates, Inc.},
  eprint        = {2102.03896},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2102.03896},
  doi           = {10.48550/arXiv.2102.03896},
  note          = {arXiv:2102.03896v1 [cs.AI]},
}
@misc{ziegler2020finetuning,
  title         = {Fine-Tuning Language Models from Human Preferences},
  author        = {Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
  year          = {2020},
  eprint        = {1909.08593},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1909.08593},
  doi           = {10.48550/arXiv.1909.08593},
  note          = {arXiv:1909.08593v2 [cs.CL]},
}
@misc{zou2023universal,
  title         = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
  author        = {Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
  year          = {2023},
  eprint        = {2307.15043},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
}
@inproceedings{rlhf_christiano,
  author        = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages         = {},
  publisher     = {Curran Associates, Inc.},
  title         = {Deep Reinforcement Learning from Human Preferences},
  url           = {https://proceedings.neurips.cc/paper\_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
  volume        = {30},
  year          = {2017},
}
@article{jaques2019way,
  title         = {Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
  author        = {Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  journal       = {arXiv preprint arXiv:1907.00456},
  year          = {2019},
}
@article{li2016deep,
  title         = {Deep reinforcement learning for dialogue generation},
  author        = {Li, Jiwei and Monroe, Will and Ritter, Alan and Galley, Michel and Gao, Jianfeng and Jurafsky, Dan},
  journal       = {arXiv preprint arXiv:1606.01541},
  year          = {2016},
}
@article{trpo,
  author        = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  title         = {Trust Region Policy Optimization},
  journal       = {CoRR},
  year          = {2015},
  url           = {http://arxiv.org/abs/1502.05477v5},
  abstract      = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  eprint        = {1502.05477},
  primaryclass  = {cs.LG},
}
@article{levine18inference,
  author        = {Levine, Sergey},
  url           = {http://arxiv.org/abs/1805.00909v3},
  archiveprefix = {arXiv},
  eprint        = {1805.00909v3},
  file          = {:papers/RL/levine18inference.pdf:PDF},
  primaryclass  = {cs.LG},
  title         = {Reinforcement Learning and Control As Probabilistic Inference: Tutorial and Review},
  year          = {2018},
}
@article{abdolmaleki18mpo,
  author        = {Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  title         = {Maximum a Posteriori Policy Optimisation},
  journal       = {CoRR},
  year          = {2018},
  url           = {http://arxiv.org/abs/1806.06920v1},
  abstract      = {We introduce a new algorithm for reinforcement learning called Maximum aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings while achieving similar or better final performance.},
  archiveprefix = {arXiv},
  eprint        = {1806.06920},
  primaryclass  = {cs.LG},
}
@article{ppo,
  author        = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  title         = {Proximal Policy Optimization Algorithms},
  journal       = {CoRR},
  year          = {2017},
  url           = {http://arxiv.org/abs/1707.06347v2},
  abstract      = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  eprint        = {1707.06347},
  primaryclass  = {cs.LG},
}
@misc{starling2023,
  title         = {Starling-{7B}: Improving LLM Helpfulness \& Harmlessness with RLAIF},
  author        = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
  month         = nov,
  year          = {2023},
}
@article{Strathern_1997,
  title         = {`Improving ratings': audit in the British University system},
  volume        = {5},
  number        = {3},
  journal       = {European Review},
  author        = {Strathern, Marilyn},
  year          = {1997},
  pages         = {305â€“321},
}
@misc{wierman2013catastrophes,
  author       = {Adam Wierman},
  title        = {Catastrophes, Conspiracies, and Subexponential Distributions (Part II)},
  howpublished = {\url{https://rigorandrelevance.wordpress.com/2013/12/17/catastrophes-conspiracies-and-subexponential-distributions-part-ii/}},
  year         = {2013},
  note         = {Accessed: 2024-06-26},
}

<div></div>
